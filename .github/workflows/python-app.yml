# This workflow will install Python dependencies, run tests and lint with a single version of Python
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python

name: Python application

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

permissions:
  contents: read

jobs:
  build:

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 pytest
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: Lint with flake8
      run: |
        # stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    - name: Test with pytest
      run: |
        pytest

import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import csv

# Define the URL of the website to crawl
base_url = 'https://example.com'  # Replace with the target website URL

# Function to check if a URL is external
def is_external(url, domain):
    parsed_url = urlparse(url)
    return parsed_url.netloc != domain

# Initialize a list to store broken links
broken_links = []

# Send an initial HTTP GET request to the base URL
response = requests.get(base_url)

# Check if the request was successful (status code 200)
if response.status_code == 200:
    # Parse the HTML content of the page
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Get the domain of the base URL
    base_domain = urlparse(base_url).netloc
    
    # Find all anchor tags (links) in the HTML
    links = soup.find_all('a', href=True)
    
    for link in links:
        # Get the absolute URL
        absolute_url = urljoin(base_url, link['href'])
        
        # Check if the link is external to the base domain
        if is_external(absolute_url, base_domain):
            # Send a HEAD request to check the status code
            link_response = requests.head(absolute_url)
            
            # Check if the status code is not 200 (broken link)
            if link_response.status_code != 200:
                broken_links.append((absolute_url, link_response.status_code))
    
    # Export broken links to a CSV file
    with open('broken_links.csv', 'w', newline='') as csv_file:
        csv_writer = csv.writer(csv_file)
        csv_writer.writerow(['URL', 'Status Code'])
        
        for link, status_code in broken_links:
            csv_writer.writerow([link, status_code])
    
    print(f'{len(broken_links)} broken links found. Data exported to broken_links.csv.')
else:
    print('Failed to retrieve data from the website.')
